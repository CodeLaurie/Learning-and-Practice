问题,解答,URL
常见的损失函数,均方误差、交叉熵损失、hinge损失、softmax交叉熵损失,
XGBoost的特征重要度如何计算,权重、增益、覆盖、总增益、总覆盖,
随机森林,多个决策树构成，集成学习bagging，分类-多数投票，回归-预测结果的平均值,
梯度提升树,GBDT,
树模型,树的数量、深度、分割所需的最小样本数、叶子结点的最小样本数、考虑的特征数量、学习率、子样本比率、正则化项,
防止过拟合,数据增强、正则化、早停、降低模型复杂度、dropout、集成、交叉验证、参数共享、稀疏表示,
集成学习,bagging，boosting，stacking，voting,
正则化,"L1, L2正则，L1-特征选择，L2-模型参数限制",
rag,通过检索找到与问题最相关的信息，然后基于这些信息生成答案,
模型微调,学习率，epoch，batch size，正则化,
" 在itemcf中如何引入时间间隔信息和位置间隔信息",时间衰减模型，时间窗口，位置权重，地理分割,
" 基于商品属性embedding的召回怎么做的",将商品的各种属性（如类别、品牌、价格等）转换为嵌入向量（Embedding），并利用这些嵌入向量来计算商品之间的相似性，从而实现召回,
Annoy的原理,基于树的方法来构建索引，并利用这些树来进行高效的最近邻搜索,
基于w2v的ANN怎么做的,词向量嵌入+高效搜索,
gpt3和gpt4的区别,模型参数，性能准确，多模态，,
bert和transformer的区别,bert仅用编码，transformer是编码器-解码器,
激活函数有哪些,sigmoid，Relu，softmax，Tanh,
llma3,"8B,70B，多种数据",
怎么做关键帧识别,帧间拆分，聚类，阀值,
Catalyst 优化器,使用一个通用库生成树,
sigmoid 激活函数,非线形，隐藏层，输出层，0和1之间,
根据二叉树的遍历确定二叉树,前序-第一个是根节点→中序找到根节点,
Adaboost和GBDT模型,集成学习boosting,
SVM模型,支持向量机，监督学习，分类和回归，,
梯度弥散,反向传播，梯度下降，Relu，批量归一化，学习率调整,
手写attention,"序列数据处理任务（nlp或者时间序列分析）,一个全连接层+softmax",
手写DCN,特征的交叉组合，大量稀疏数据，一个交叉层+几个深度层,
对数损失函数,二分类和多分类问题,
处理文本和图像信息通常使用的架构，为什么要这么构造,理解语义-可变长度输入-上下文相关表示,
文本和图像的表征如何学习,embedding,
度量相似度有哪些方法,余弦相似度，欧氏距离，曼哈顿距离，皮尔逊相关系数,
度量概率分布有哪些方法,KL散度，JS散度，海林格距离，巴氏距离,
手撕KL散度，KL散度是否对称,不对称，一个真实概率乘以这个概率除以另外一个概率的对数,
讲一下Word2Vector的两种模型,连续词袋模型，skip-gram,
如何进行Sentence2Vector,"平均词向量，TF-IDF,Doc2VEC,RNN,LSTM,transformer，bert",
自回归模型和双向预测模型的区别,时间序列，自回归是单向，双向模型是双向,
如何进行冷启动，有哪些方法,用户（调查，人口统计学），项目（内容基推荐），系统（类似的系）,
如何结合多模态信息进行冷启动,内容基，外部元数据，深度学习，迁移学习，用户互动，强化学习,
召回粗排精排的作用区别和对应的模型结构,减少数据量，增加复杂度,
如何解决精排和粗排排序结果不一致问题,特征和数据的一致性，模型一致性，端到端训练，反馈机制，系统级集成,
向量检索用哪些方法,精确检索，ann，深度学习,
判断字符串中是否存在目标子串最优的时间复杂度算法,"KMP,",
设计一个论文判重的算法,文本预处理-特征提取-相似度计算,
如何在模型里添加约束,特征工程，损失函数，后处理,
电商场景有什么特点可以建模,用户行为分析，产品推荐系统，库存，市场分析，风险管理,
双十一大促怎么预判销量,历史数据，时间序列，回归模型，集成学习，深度学习，lstm，rnn,
在大促期间怎么进行推荐,,
序列推荐都有什么方法,马尔可夫链，因子，rnn，lstm，gru，attention，gnn,
怎么解决SIM的不一致性,特征加权，领域相似度，动态调整，深度学习，反馈循环,
在搜索场景下如何使用用户的行为序列,查询扩展，个性化搜索，相关搜索，搜索历史记录，实时反馈，上下文感知,
搜索和推荐场景的区别,搜索：目标明确，推荐：目标不明确,
大模型怎么用到推荐场景下,特征提取，模型融合,
多模态融合怎么做,特征层级，神经网络，注意力，模型融合，联合训练，迁移学习,
机器学习框架,tensorflow（精态图），pytorch（动态图），keras，sklearn，,
神经网络模型设计,输入-隐藏-输出，激活函数，损失函数，优化器，超参数,
超参数优化,网格搜索、随机搜索、贝叶斯优化、遗传算法、超带宽优化,
深度学习,神经网络，cnn，rnn，lstm,
增强学习,强化学习，agent，reinforcement learning fom human feedback,
模式识别,识别数据中的模式和规律,
概率统计,概率，随机变量，概率分布，期望值，方差，正态分布,
最优化算法,梯度下降,
spark,分布式计算系统，rdd，dataframe，spark sql，mllib,
XGBoost,分布式梯度提升库,
Caffe,速度，模块化，表达力,
Tensorflow,api高灵活性，keras,
深度学习框架对比,,
"rf,adaboost,xgboost",,
auc,roc下面的曲线面积，0-1,
大模型用推荐,,
粗排精排重排,,
分类问题的交叉熵损失,交叉熵损失，hinge loss，kl散度，平方损失,
ALS-最小二乘法,,
早停,正则化，防止过拟合，耐心参数,
线下提升线上效果不好,样本、评估指标、环境,
XGBoost和GBDT是什么？有什么区别？,xgboost加入正则项防止过拟合、采用二阶泰勒展开、多种基分类器，自动学习缺失值处理,
偏差与方差。延伸知识（集成学习的三种方式: Bagging、Boosting、Stacking）。,偏差是预测的期望与真实的区别，方差是偏离真实的程度,
随机森林是什么？,基于bagging（强模型），回归-预测值的平均，分类-投票,
Word2Vec常见的加速训练的方式有哪些？,高频词抽样、负采样、层级softmax,
LightGBM是什么？,,
AdaBoost思想？,对分类错误的样本提高权重,
损失函数无法求导该怎么样进行优化？,A: 使用次梯度。,
召回模型中，评价指标怎么设定？,召回率、精确率、F1,
多路召回的作用？,交叉与互补，满足用户兴趣多元化。,
 Skip-grim和Cbow的区别？Skip-grim优势在哪？,,
 FTRL是什么？,,
梯度下降方法,基于权重的增量更新、基于动量的增量更新,
推荐系统中常见的Embedding方法有哪些？,,
Embedding与推荐系统的结合。,,
特征筛选方法。,,
bert和sentence-bert,,
树模型,集成学习，随机森林，决策树，gbdt,
大模型用于推荐,llm+推荐，llm as 推荐,
prompt技巧,,
精排特征交叉 -- 总体架构,特征交互、用户行为序列建模、embedding表征学习、多任务学习,https://xieyangyi.blog.csdn.net/article/details/135350439?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogOpenSearchComplete%7ERate-3-135350439-blog-139296412.235%5Ev43%5Epc_blog_bottom_relevance_base8&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogOpenSearchComplete%7ERate-3-135350439-blog-139296412.235%5Ev43%5Epc_blog_bottom_relevance_base8&utm_relevant_index=4