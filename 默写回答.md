# 回顾、复习、默写回答
- transformer的架构介绍一下
    - 有两大块，encoder和decoder
    - encoder里面有多个重复的块（文章是六个），每一个大块有两部分组成：multi-head attention和forward
    - decoder里面也有多个重复的block（文章是六个），每一个block有三部分组成：masked multi-head attention, 和encoder交互的multi-head attention,和forward
    - position embedding(位置编码)的作用是可以关注到多个空间的映射，从而表示单词在文本中出现的位置，采用公式计算：PE(2i)=sin（pos/10002i/d）
    - multi-head attention是由多个self- attention构成的
        - self attention计算的时候要用到Q(query，查询),K（keys，键）,V（value，值）
            - Q,K,V计算是用输入矩阵x分别乘以线性变换矩阵WQ,WK,WV得到的
            - 拿到Q,K,V之后可以计算attention分数attention（Q,K,V）=softmaxt（QKT/√dk）V，dk是向量维度
            - 除以dk的原因是防止内积过大
        - 多个self attention得到多个输出矩阵，然后multi-head attention会把这些矩阵contac在一起，传入linear层得到最终的输出矩阵（和输入矩阵的维度一样）
    - encoder里面的add是残差连接，解决多层网络训练的问题，只关注当前差异的部分，norm是layer normalization，作用是将每一层的输出变成方差一样，可以加快收敛
    - feed forwad有两层全连接层，一层是有激活函数relu，第二层不使用激活函数，max（0，xw1+b1）w2+b2
    - decoder 里面有两个multi-head attention
        - 第一个要mask一部分
        - 第二个kv使用encoder的矩阵，Q使用上一个decoder block的输出
        - 最后有个softmax计算概率

- Batch normalization
    - 把偏的分布拉回到标准的0，1正态分布，敏感（输入的小变化导致损失函数的大变化），保持数据同分布
    - 作用
        - 加快网络的训练和收敛的速度：因为保持了每一层数据的分布一样
        - 控制梯度爆炸，防止梯度消失： 通常加在全连接层和激活函数之间，保持在0附近，激活函数在0附近大部分都有较大的变化（不会梯度消失），可以使得权重不会太大就不会梯度爆炸
        - 防止过拟合： 使得一个mini batch的所有样本都关联起来，不会从一个训练样本生成一个确切的值

- Rag的原理、过程
    - 检索增强生成，可以解决模型幻觉问题
    - LLM+文本向量数据库的问答：
        - 加载文件->读取文件->文本分割->文本向量化->问句向量化->匹配最相似的topk->匹配文本加入到prompt中->提交llm生成回答
        - 核心是embedding，计算相似度可以用cos similarity，欧式距离等等

- 微调的方法
    - sft：self-training(supervised) fine-tuning
        - 全参数微调：对所有的参数进行微调
        - 部分参数微调：选择性更新某些部分的权重
            - lora：在模型权重中加入一个低秩矩阵
            - p tuning: 基于prompt tuning，修改与prompt相关的参数
            - qlora： 结合lora（低秩调整）和量化，高效友好的微调
        - 冻结监督（freeze）：部分或者全部的参数冻结，对部分层进行训练
    - 指令微调：instruction tuning，增强对指令的理解

- llm agent相关
    - llm+planing+memory+tools+action，让大模型实现任务自动化
    - 构建agent：定义工具，定义agent，agent执行

- 自回归
    - 处理时间序列
    - 递归，条件概率建模

- Retrieval系统的一些模型利用什么数据怎么训练的。
    - 纯文本（BM25,TF-ID）
    - 查询-文档对（双塔模型，bert）
    - 用户-交互数据（强化学习）

- 正则化的方法
    - 正则化是为了防止过拟合，正则性越高，函数越光滑-可导性
    - 有L1正则，L2正则（在原来的损失函数后面加上正则，l1-绝对值，l2-平方）
    - drop out正则，随机丢取一定数量的神经网络节点，用于修正过拟合状态
    - 数据集增强正则，在原有的数据集上做数据增强
    - 早停正则，未达到过拟合就停止训练
    - 基于数据：数据增强，drop out；基于网络结构：共享参数，激活函数，噪音模型，多任务学习，模型选择；基于正则化项：l1、l2；基于优化：早停，初始化方法，优化方法

- 过拟合怎么处理
    - 模型在训练集上表现好，但是在测试集上表现差
    - 原因：模型复杂度过高，参数过多；数据集比较小；训练集和测试集的分布不一致（噪声数据过大、测试集的标签不对应）
    - 解决：
        - 降低模型的复杂度：移除层
        - 增加数据集
        - 数据增强
        - 正则化
        - drop out
        - 早停
        - 重新清洗数据
        - 集成学习
        - 批量正则化（BN）：将网络的每一层之间加上将神经元的权重调成标准正态分布的正则化层

- bn ln 具体公式和区别
    - bn：批量归一化，考虑的是一层不同样本之间的均值和方差，适用于大批量数据，小批量表现不好
        ```python
            import numpy as np

            def batch_normalization(X, gamma, beta, eps=1e-5):
                """
                对输入数据进行批量归一化
                
                参数:
                X: 输入数据，形状为 (batch_size, num_features)
                gamma: 缩放参数，形状为 (num_features,)
                beta: 平移参数，形状为 (num_features,)
                eps: 防止除零的一个小值，默认值为 1e-5
                
                返回:
                out: 归一化后的输出数据
                """
                mean = np.mean(X, axis=0)
                variance = np.var(X, axis=0)

                X_norm = (X - mean) / np.sqrt(variance + eps)
                out = gamma * X_norm + beta

                return out
    - ln：层归一化，考虑的是同一层不同神经元的均值和方差，单样本或者小批量表现好
        ```python
        import numpy as np

        def layer_normalization(X,gamma,beta,eps=1e-5):
            """
            X: 输入数据，形状为 (batch_size, num_features)
            gamma: 缩放参数，形状为 (num_features,)
            beta: 平移参数，形状为 (num_features,)
            eps: 防止除零的一个小值
            """

            mean=np.mean(X,axis=0)
            variance=np.var(X,axis=0)

            X_norm=(X-mean)/np.sqrt(variance+eps)
            out=gamma*X_norm+bata

            return out

    - BN：在批次维度上归一化，即计算每个特征在整个批次上的均值和方差
    - LN：在层维度上归一化，即计算每个样本的均值和方差。
    - BN：对于每个特征，计算其在整个批次中的均值和方差，然后进行归一化。
    - LN：对于每个样本，计算其所有特征的均值和方差，然后进行归一化。

- 手撕 attention
    - 步骤
        - 计算Q,K,V矩阵
        - 计算Q,K的点积
        - 对点积进行缩放
        - softmax获得注意力权重
        - 使用注意力权重对V进行加权求和，得到输出
    - 代码
        ```python
        import numpy as np

        def attention(Q,K,V,mask=None):
            
            # 1. 计算Q,K,V
            d_k=Q.shape[-1]
            scores=np.matmul(Q,K.T)/np.sqrt(d_k)

            if mask is not None:
                scores=np.where(mask,scores,-1e9)

            attention_weights=softmax(scores)
            output=np.matmul(attention_weights,V)
            
            return output,attention_weights
        
        def softmax(x):
            exp_x=np.exp(x-np.max(x,axis=-1,keepdims=True))

            return exp_x/np.sum(exp_x,axis=-1,keepdims=True)

        #示例
        np.random.seed(0)  # 为了可重复性
        Q = np.random.rand(3, 4)  # 查询矩阵，形状为 (num_queries, d_k)
        K = np.random.rand(3, 4)  # 键矩阵，形状为 (num_keys, d_k)
        V = np.random.rand(3, 4)  # 值矩阵，形状为 (num_values, d_v)

        output, attention_weights = attention(Q, K, V)

        print("Output:\n", output)
        print("Attention Weights:\n", attention_weights)

- MMOE和STAR
    - MMOE:多任务学习框架

- 强化学习算法与知识图谱算法如何应用在推荐系统里
    - 强化学习算法
        - 基于策略的强化学习算法
        - 基于价值函数的强化学习算法
    - 知识图谱算法
        - 基于图的推理算法
        - 基于图的表示学习算法
        - 基于图的优化算法
    - 应用
        - 强化学习算法：强化学习算法可以用于推荐系统中，通过学习用户行为和偏好，优化推荐结果。
            - 推荐系统作为智能体，用户作为环境
                - State：Agent对Environment的观测，即用户的意图和所处场景。
                - Action：以List-Wise粒度对推荐列表做调整，考虑长期收益对当前决策的影响。
                - Reward：根据用户反馈给予Agent相应的奖励，为业务目标直接负责。
                - P(s,a)：Agent在当前State s下采取Action a的状态转移概率。
            - 状态建模
                - 用户意图：用户意图可以作为推荐系统的状态，包括用户的目标、兴趣、行为等。
                - 用户场景：用户场景可以作为推荐系统的状态，包括用户所处的地理位置、设备类型等。
                - 用户行为：用户行为可以作为推荐系统的状态，包括用户点击、浏览、购买等行为。
            - 动作建模
                - 推荐列表调整：推荐列表调整可以作为推荐系统的动作，包括调整推荐列表的顺序、添加或删除推荐项等。
                - 长期收益优化：长期收益优化可以作为推荐系统的动作，包括调整推荐列表的权重、调整推荐项的优先级等。
                - 奖励反馈：奖励反馈可以作为推荐系统的奖励，包括用户点击、浏览、购买等行为的反馈。
            - 强化学习算法：基于策略的强化学习算法可以用于推荐系统中，通过学习用户行为和偏好，优化推荐结果。
                - 策略：策略是指Agent在当前状态s下采取Action a的概率分布。
                - 价值函数：价值函数是指Agent在当前状态s下采取Action a的预期奖励。
                - 策略优化：策略优化是指通过优化策略，最大化价值函数，从而优化推荐结果。

        - 知识图谱算法：知识图谱算法可以用于推荐系统中，通过构建用户兴趣的知识图谱，进行推荐。

            - 依次学习：首先使用知识图谱特征学习得到实体向量和关系向量，然后将这些低维向量引入推荐系统，学习得到用户向量和物品向量，DKN
            - 联合学习：将知识图谱特征学习和推荐算法的目标函数结合，使用端到端（end-to-end）的方法进行联合学习,CKE,Ripple Network
            - 交替学习：将知识图谱特征学习和推荐算法视为两个分离但又相关的任务，使用多任务学习（multi-task learning）的框架进行交替学习,MKR

- 推荐系统中的知识图谱如何建立
    - 知识图谱的构建（实体，关系，属性）
        - 实体：实体是知识图谱的基本单元，表示现实世界中的对象，如人、物、地点等。
        - 关系：关系是实体之间的联系，表示实体之间的语义关系，如“人”和“物”之间有“拥有”的关系，“地点”和“人”之间有“位于”的关系。
        - 属性：属性是实体的一种特征，表示实体的具体属性，如“人”的年龄、性别，“地点”的经纬度等。
    - 知识图谱的表示（向量，图）
        - 向量：知识图谱的表示可以采用向量的方式，将实体和关系映射为低维的向量，以便进行相似度计算和推理。
        - 图：知识图谱的表示可以采用图的方式，将实体和关系表示为图中的节点和边，以便进行图的遍历和推理。
    - 知识图谱的构建流程
        - 数据收集：从各种来源收集实体、关系和属性数据，如百科全书、社交媒体、数据库等。
        - 数据清洗：对收集到的数据进行清洗和处理，去除重复、无效的数据。
        - 数据融合：将不同来源的数据进行融合，构建完整的知识图谱。
        - 知识抽取：使用自然语言处理技术，从文本中提取实体、关系和属性。
        - 知识表示：将提取到的知识表示为向量或图的形式，以便进行相似度计算和推理。
        - 知识存储：将知识图谱存储到数据库或图数据库中，以便进行查询和更新。
        - 知识更新：定期更新知识图谱，以适应不断变化的世界。
- 推荐系统中的知识图谱如何应用
    - 实体和关系的抽取和表示
        - 实体和关系的抽取
            - 实体抽取：使用分词、命名实体识别等技术，从文本中提取出实体。
            - 关系抽取：使用序列标注、条件随机场等技术，从文本中提取出实体之间的关系。
        - 实体和关系的表示
            - 实体表示：使用词向量、向量空间模型等技术，将实体表示为向量形式。
            - 关系表示：使用词向量、向量空间模型等技术，将关系表示为向量形式。
    - 代码
        ```python
            import networkx as nx
            import numpy as np

            # 构建知识图谱
            G = nx.DiGraph()

            # 添加实体
            entities = ['电影A', '电影B', '电影C', '用户1', '用户2', '用户3']
            for entity in entities:
                G.add_node(entity)

            # 添加关系
            relations = [
                ('电影A', '出演', '演员1'),
                ('电影A', '导演', '导演1'),
                ('电影B', '出演', '演员1'),
                ('电影B', '导演', '导演2'),
                ('电影C', '出演', '演员2'),
                ('电影C', '导演', '导演3'),
                ('用户1', '喜欢', '电影A'),
                ('用户2', '喜欢', '电影B'),
                ('用户3', '喜欢', '电影C'),
            ]

            for relation in relations:
                G.add_edge(*relation)

            # 计算实体相似度
            similarity = nx.graph_similarity(G, 'user', 'movie', weight='weight')

            # 推荐列表
            recommended_movies = []
            for user in similarity.keys():
                similarity[user] = sorted(similarity[user], key=lambda x: similarity[user][x], reverse=True)
                recommended_movies.append(similarity[user][:5])

            print(recommended_movies)

- tensorflow如何创建变量
    ```python
        import tensorflow as tf

        # 创建一个变量
        state = tf.Variable(0, name='counter')

        # 创建一个操作，用于增加变量的值
        one = tf.constant(1)
        new_value = tf.add(state, one)
        update = tf.assign(state, new_value)

        # 启动图
        init_op = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init_op)
            for _ in range(3):
                sess.run(update)
                print(sess.run(state))




