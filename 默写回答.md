# 回顾、复习、默写回答
- transformer的架构介绍一下
    - 有两大块，encoder和decoder
    - encoder里面有多个重复的块（文章是六个），每一个大块有两部分组成：multi-head attention和forward
    - decoder里面也有多个重复的block（文章是六个），每一个block有三部分组成：masked multi-head attention, 和encoder交互的multi-head attention,和forward
    - position embedding(位置编码)的作用是可以关注到多个空间的映射，从而表示单词在文本中出现的位置，采用公式计算：PE(2i)=sin（pos/10002i/d）
    - multi-head attention是由多个self- attention构成的
        - self attention计算的时候要用到Q(query，查询),K（keys，键）,V（value，值）
            - Q,K,V计算是用输入矩阵x分别乘以线性变换矩阵WQ,WK,WV得到的
            - 拿到Q,K,V之后可以计算attention分数attention（Q,K,V）=softmaxt（QKT/√dk）V，dk是向量维度
            - 除以dk的原因是防止内积过大
        - 多个self attention得到多个输出矩阵，然后multi-head attention会把这些矩阵contac在一起，传入linear层得到最终的输出矩阵（和输入矩阵的维度一样）
    - encoder里面的add是残差连接，解决多层网络训练的问题，只关注当前差异的部分，norm是layer normalization，作用是将每一层的输出变成方差一样，可以加快收敛
    - feed forwad有两层全连接层，一层是有激活函数relu，第二层不使用激活函数，max（0，xw1+b1）w2+b2
    - decoder 里面有两个multi-head attention
        - 第一个要mask一部分
        - 第二个kv使用encoder的矩阵，Q使用上一个decoder block的输出
        - 最后有个softmax计算概率

- Batch normalization
    - 把偏的分布拉回到标准的0，1正态分布，敏感（输入的小变化导致损失函数的大变化），保持数据同分布
    - 作用
        - 加快网络的训练和收敛的速度：因为保持了每一层数据的分布一样
        - 控制梯度爆炸，防止梯度消失： 通常加在全连接层和激活函数之间，保持在0附近，激活函数在0附近大部分都有较大的变化（不会梯度消失），可以使得权重不会太大就不会梯度爆炸
        - 防止过拟合： 使得一个mini batch的所有样本都关联起来，不会从一个训练样本生成一个确切的值

- Rag的原理、过程
    - 检索增强生成，可以解决模型幻觉问题
    - LLM+文本向量数据库的问答：
        - 加载文件->读取文件->文本分割->文本向量化->问句向量化->匹配最相似的topk->匹配文本加入到prompt中->提交llm生成回答
        - 核心是embedding，计算相似度可以用cos similarity，欧式距离等等

- 微调的方法
    - sft：self-training(supervised) fine-tuning
        - 全参数微调：对所有的参数进行微调
        - 部分参数微调：选择性更新某些部分的权重
            - lora：在模型权重中加入一个低秩矩阵
            - p tuning: 基于prompt tuning，修改与prompt相关的参数
            - qlora： 结合lora（低秩调整）和量化，高效友好的微调
        - 东冻结监督（freeze）：部分或者全部的参数冻结，对部分层进行训练
    - 指令微调：instruction tuning，增强对指令的理解

- llm agent相关
    - llm+planing+memory+tools+action，让大模型实现任务自动化
    - 构建agent：定义工具，定义agent，agent执行

- 自回归
    - 处理时间序列
    - 递归，条件概率建模

- Retrieval系统的一些模型利用什么数据怎么训练的。
    - 纯文本（BM25,TF-ID）
    - 查询-文档对（双塔模型，bert）
    - 用户-交互数据（强化学习）

- 正则化的方法
    - 正则化是为了防止过拟合，正则性越高，函数越光滑-可导性
    - 有L1正则，L2正则（在原来的损失函数后面加上正则，l1-绝对值，l2-平方）
    - drop out正则，随机丢取一定数量的神经网络节点，用于修正过拟合状态
    - 数据集增强正则，在原有的数据集上做数据增强
    - 早停正则，未达到过拟合就停止训练
    - 基于数据：数据增强，drop out；基于网络结构：共享参数，激活函数，噪音模型，多任务学习，模型选择；基于正则化项：l1、l2；基于优化：早停，初始化方法，优化方法

- 过拟合怎么处理
    - 模型在训练集上表现好，但是在测试集上表现差
    - 原因：模型复杂度过高，参数过多；数据集比较小；训练集和测试集的分布不一致（噪声数据过大、测试集的标签不对应）
    - 解决：
        - 降低模型的复杂度：移除层
        - 增加数据集
        - 数据增强
        - 正则化
        - drop out
        - 早停
        - 重新清洗数据
        - 集成学习
        - 批量正则化（BN）：将网络的每一层之间加上将神经元的权重调成标准正态分布的正则化层

- bn ln 具体公式和区别
    - bn：批量归一化，考虑的是一层不同样本之间的均值和方差，适用于大批量数据，小批量表现不好
        ```python
            import numpy as np

            def batch_normalization(X, gamma, beta, eps=1e-5):
                """
                对输入数据进行批量归一化
                
                参数:
                X: 输入数据，形状为 (batch_size, num_features)
                gamma: 缩放参数，形状为 (num_features,)
                beta: 平移参数，形状为 (num_features,)
                eps: 防止除零的一个小值，默认值为 1e-5
                
                返回:
                out: 归一化后的输出数据
                """
                mean = np.mean(X, axis=0)
                variance = np.var(X, axis=0)

                X_norm = (X - mean) / np.sqrt(variance + eps)
                out = gamma * X_norm + beta

                return out
    - ln：层归一化，考虑的是同一层不同神经元的均值和方差，单样本或者小批量表现好
        ```python
        import numpy as np

        def layer_normalization(X,gamma,beta,eps=1e-5):
            """
            X: 输入数据，形状为 (batch_size, num_features)
            gamma: 缩放参数，形状为 (num_features,)
            beta: 平移参数，形状为 (num_features,)
            eps: 防止除零的一个小值
            """

            mean=np.mean(X,axis=0)
            variance=np.var(X,axis=0)

            X_norm=(X-mean)/np.sqrt(variance+eps)
            out=gamma*X_norm+bata

            return out

    - BN：在批次维度上归一化，即计算每个特征在整个批次上的均值和方差
    - LN：在层维度上归一化，即计算每个样本的均值和方差。
    - BN：对于每个特征，计算其在整个批次中的均值和方差，然后进行归一化。
    - LN：对于每个样本，计算其所有特征的均值和方差，然后进行归一化。

- 手撕 attention
    - 步骤
        - 计算Q,K,V矩阵
        - 计算Q,K的点积
        - 对点积进行缩放
        - softmax获得注意力权重
        - 使用注意力权重对V进行加权求和，得到输出
    - 代码
        ```python
        import numpy as np

        def attention(Q,K,V,mask=None):
            
            # 1. 计算Q,K,V
            d_k=Q.shape[-1]
            scores=np.matmul(Q,K.T)/np.sqrt(d_k)

            if mask is not None:
                scores=np.where(mask,scores,-1e9)

            attention_weights=softmax(scores)
            output=np.matmul(attention_weights,V)
            
            return output,attention_weights
        
        def softmax(x):
            exp_x=np.exp(x-np.max(x,axis=-1,keepdims=True))

            return exp_x/np.sum(exp_x,axis=-1,keepdims=True)

        #示例
        np.random.seed(0)  # 为了可重复性
        Q = np.random.rand(3, 4)  # 查询矩阵，形状为 (num_queries, d_k)
        K = np.random.rand(3, 4)  # 键矩阵，形状为 (num_keys, d_k)
        V = np.random.rand(3, 4)  # 值矩阵，形状为 (num_values, d_v)

        output, attention_weights = attention(Q, K, V)

        print("Output:\n", output)
        print("Attention Weights:\n", attention_weights)

- MMOE和STAR
    - MMOE:多任务学习框架




