# 回顾、复习、默写回答
- transformer的架构介绍一下
    - 有两大块，encoder和decoder
    - encoder里面有多个重复的块（文章是六个），每一个大块有两部分组成：multi-head attention和forward
    - decoder里面也有多个重复的block（文章是六个），每一个block有三部分组成：masked multi-head attention, 和encoder交互的multi-head attention,和forward
    - position embedding(位置编码)的作用是可以关注到多个空间的映射，从而表示单词在文本中出现的位置，采用公式计算：PE(2i)=sin（pos/10002i/d）
    - multi-head attention是由多个self- attention构成的
        - self attention计算的时候要用到Q(query，查询),K（keys，键）,V（value，值）
            - Q,K,V计算是用输入矩阵x分别乘以线性变换矩阵WQ,WK,WV得到的
            - 拿到Q,K,V之后可以计算attention分数attention（Q,K,V）=softmaxt（QKT/√dk）V，dk是向量维度
            - 除以dk的原因是防止内积过大
        - 多个self attention得到多个输出矩阵，然后multi-head attention会把这些矩阵contac在一起，传入linear层得到最终的输出矩阵（和输入矩阵的维度一样）
    - encoder里面的add是残差连接，解决多层网络训练的问题，只关注当前差异的部分，norm是layer normalization，作用是将每一层的输出变成方差一样，可以加快收敛
    - feed forwad有两层全连接层，一层是有激活函数relu，第二层不使用激活函数，max（0，xw1+b1）w2+b2
    - decoder 里面有两个multi-head attention
        - 第一个要mask一部分
        - 第二个kv使用encoder的矩阵，Q使用上一个decoder block的输出
        - 最后有个softmax计算概率

- Batch normalization
    - 把偏的分布拉回到标准的0，1正态分布，敏感（输入的小变化导致损失函数的大变化），保持数据同分布
    - 作用
        - 加快网络的训练和收敛的速度：因为保持了每一层数据的分布一样
        - 控制梯度爆炸，防止梯度消失： 通常加在全连接层和激活函数之间，保持在0附近，激活函数在0附近大部分都有较大的变化（不会梯度消失），可以使得权重不会太大就不会梯度爆炸
        - 防止过拟合： 使得一个mini batch的所有样本都关联起来，不会从一个训练样本生成一个确切的值

- Rag的原理、过程
    - 检索增强生成，可以解决模型幻觉问题
    - LLM+文本向量数据库的问答：
        - 加载文件->读取文件->文本分割->文本向量化->问句向量化->匹配最相似的topk->匹配文本加入到prompt中->提交llm生成回答
        - 核心是embedding，计算相似度可以用cos similarity，欧式距离等等

- 微调的方法
    - sft：self-training(supervised) fine-tuning
        - 全参数微调：对所有的参数进行微调
        - 部分参数微调：选择性更新某些部分的权重
            - lora：在模型权重中加入一个低秩矩阵
            - p tuning: 基于prompt tuning，修改与prompt相关的参数
            - qlora： 结合lora（低秩调整）和量化，高效友好的微调
        - 东冻结监督（freeze）：部分或者全部的参数冻结，对部分层进行训练
    - 指令微调：instruction tuning，增强对指令的理解

- llm agent相关
    - llm+planing+memory+tools+action，让大模型实现任务自动化
    - 构建agent：定义工具，定义agent，agent执行

- 自回归
    - 处理时间序列
    - 递归，条件概率建模

- Retrieval系统的一些模型利用什么数据怎么训练的。
    - 纯文本（BM25,TF-ID）
    - 查询-文档对（双塔模型，bert）
    - 用户-交互数据（强化学习）

